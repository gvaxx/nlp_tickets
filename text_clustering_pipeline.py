# =============================================================================
# –ö–õ–ê–°–° –î–õ–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò –¢–ï–ö–°–¢–û–í
# =============================================================================

import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from umap.umap_ import UMAP
import hdbscan
import matplotlib.pyplot as plt
from typing import Optional, Tuple
import random

class TextClusteringPipeline:
    """
    –ü–æ–ª–Ω—ã–π pipeline –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤:
    1. –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    2. –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (UMAP)
    3. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (HDBSCAN)
    
    –í—Ö–æ–¥: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ [id, text]
    –í—ã—Ö–æ–¥: DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ [id, text, cluster_id, cluster_confidence]
    """
    
    def __init__(
        self,
        embedding_model_name: str = 'ai-forever/FRIDA',
        random_seed: int = 42
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è pipeline
        
        Args:
            embedding_model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            random_seed: —Å–∏–¥ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        """
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º seed
        random.seed(random_seed)
        np.random.seed(random_seed)
        
        self.random_seed = random_seed
        self.embedding_model_name = embedding_model_name
        
        # –ú–æ–¥–µ–ª–∏ (–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)
        self.embedding_model = None
        self.umap_model = None
        self.clusterer = None
        
        # –î–∞–Ω–Ω—ã–µ
        self.embeddings = None
        self.embeddings_reduced = None
        self.cluster_labels = None
        self.cluster_probabilities = None
    
    def _load_embedding_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if self.embedding_model is None:
            print(f"üì• –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {self.embedding_model_name}")
            self.embedding_model = SentenceTransformer(self.embedding_model_name)
    
    def create_embeddings(
        self,
        texts: list,
        batch_size: int = 32,
        use_multiprocessing: bool = True
    ) -> np.ndarray:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            texts: —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            use_multiprocessing: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
        
        Returns:
            –º–∞—Å—Å–∏–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (n_samples, embedding_dim)
        """
        self._load_embedding_model()
        
        print(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")
        
        if use_multiprocessing:
            import multiprocessing
            num_workers = multiprocessing.cpu_count()
            print(f"   –ò—Å–ø–æ–ª—å–∑—É–µ–º {num_workers} CPU —è–¥–µ—Ä")
            
            pool = self.embedding_model.start_multi_process_pool()
            
            embeddings = self.embedding_model.encode(
                texts,
                batch_size=batch_size,
                pool=pool,
                chunk_size=1000,
                show_progress_bar=True
            )
            
            self.embedding_model.stop_multi_process_pool(pool)
        else:
            embeddings = self.embedding_model.encode(
                texts,
                batch_size=batch_size,
                show_progress_bar=True,
                convert_to_numpy=True
            )
        
        self.embeddings = embeddings
        print(f"‚úÖ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embeddings.shape}")
        
        return embeddings
    
    def reduce_dimensions(
        self,
        embeddings: Optional[np.ndarray] = None,
        n_components: int = 5,
        n_neighbors: int = 15,
        min_dist: float = 0.0,
        metric: str = 'cosine'
    ) -> np.ndarray:
        """
        –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –ø–æ–º–æ—â—å—é UMAP
        
        Args:
            embeddings: –º–∞—Å—Å–∏–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç self.embeddings)
            n_components: —Ü–µ–ª–µ–≤–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
            n_neighbors: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–µ–π (–±–∞–ª–∞–Ω—Å –ª–æ–∫–∞–ª—å–Ω–æ–π/–≥–ª–æ–±–∞–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã)
            min_dist: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ç–æ—á–∫–∞–º–∏
            metric: –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
        
        Returns:
            –º–∞—Å—Å–∏–≤ —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (n_samples, n_components)
        """
        if embeddings is None:
            if self.embeddings is None:
                raise ValueError("–°–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞–π—Ç–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ create_embeddings()")
            embeddings = self.embeddings
        
        print(f"üîÑ –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —Å UMAP –¥–æ {n_components}D...")
        
        self.umap_model = UMAP(
            n_neighbors=n_neighbors,
            n_components=n_components,
            min_dist=min_dist,
            metric=metric,
            random_state=self.random_seed
        )
        
        embeddings_reduced = self.umap_model.fit_transform(embeddings)
        
        self.embeddings_reduced = embeddings_reduced
        print(f"‚úÖ –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –ø–æ—Å–ª–µ UMAP: {embeddings_reduced.shape}")
        
        return embeddings_reduced
    
    def cluster(
        self,
        embeddings: Optional[np.ndarray] = None,
        min_cluster_size: int = 50,
        min_samples: int = 10,
        metric: str = 'euclidean',
        cluster_selection_method: str = 'eom'
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é HDBSCAN
        
        Args:
            embeddings: –º–∞—Å—Å–∏–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç self.embeddings_reduced)
            min_cluster_size: –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
            min_samples: –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–µ–π –¥–ª—è core point
            metric: –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è
            cluster_selection_method: –º–µ—Ç–æ–¥ –≤—ã–±–æ—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ('eom' –∏–ª–∏ 'leaf')
        
        Returns:
            (cluster_labels, cluster_probabilities)
        """
        if embeddings is None:
            if self.embeddings_reduced is None:
                raise ValueError("–°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–º–µ–Ω–∏—Ç–µ reduce_dimensions()")
            embeddings = self.embeddings_reduced
        
        print(f"üîÑ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å HDBSCAN...")
        
        self.clusterer = hdbscan.HDBSCAN(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples,
            metric=metric,
            cluster_selection_method=cluster_selection_method,
            prediction_data=True
        )
        
        cluster_labels = self.clusterer.fit_predict(embeddings)
        cluster_probabilities = self.clusterer.probabilities_
        
        self.cluster_labels = cluster_labels
        self.cluster_probabilities = cluster_probabilities
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        n_noise = list(cluster_labels).count(-1)
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {n_clusters}")
        print(f"   –®—É–º–æ–≤—ã—Ö —Ç–æ—á–µ–∫: {n_noise} ({n_noise/len(cluster_labels)*100:.2f}%)")
        
        # –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        unique, counts = np.unique(cluster_labels, return_counts=True)
        print("\nüìä –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
        for cluster_id, count in sorted(zip(unique, counts)):
            if cluster_id == -1:
                print(f"   –®—É–º: {count}")
            else:
                print(f"   –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {count}")
        
        return cluster_labels, cluster_probabilities
    
    def fit(
        self,
        df: pd.DataFrame,
        id_col: str = 'id',
        text_col: str = 'text',
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        batch_size: int = 32,
        use_multiprocessing: bool = True,
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã UMAP
        n_components: int = 5,
        n_neighbors: int = 15,
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã HDBSCAN
        min_cluster_size: int = 50,
        min_samples: int = 10
    ) -> pd.DataFrame:
        """
        –ü–æ–ª–Ω—ã–π pipeline: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Üí UMAP ‚Üí HDBSCAN
        
        Args:
            df: –≤—Ö–æ–¥–Ω–æ–π DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ [id, text]
            id_col: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å ID
            text_col: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º
            –æ—Å—Ç–∞–ª—å–Ω—ã–µ: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞
        
        Returns:
            DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ [id, text, cluster_id, cluster_confidence]
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if id_col not in df.columns or text_col not in df.columns:
            raise ValueError(f"DataFrame –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏ '{id_col}' –∏ '{text_col}'")
        
        print("="*80)
        print("–ó–ê–ü–£–°–ö PIPELINE –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò")
        print("="*80)
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(df)}\n")
        
        # 1. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        texts = df[text_col].tolist()
        self.create_embeddings(
            texts,
            batch_size=batch_size,
            use_multiprocessing=use_multiprocessing
        )
        
        # 2. –£–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        self.reduce_dimensions(
            n_components=n_components,
            n_neighbors=n_neighbors
        )
        
        # 3. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        self.cluster(
            min_cluster_size=min_cluster_size,
            min_samples=min_samples
        )
        
        # 4. –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result_df = df[[id_col, text_col]].copy()
        result_df['cluster_id'] = self.cluster_labels
        result_df['cluster_confidence'] = self.cluster_probabilities
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
        result_df = result_df.sort_values('cluster_id').reset_index(drop=True)
        
        print("\n" + "="*80)
        print("‚úÖ PIPELINE –ó–ê–í–ï–†–®–Å–ù")
        print("="*80)
        
        return result_df
    
    def predict(
        self,
        new_texts: list
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è –Ω–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            new_texts: —Å–ø–∏—Å–æ–∫ –Ω–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
        
        Returns:
            (cluster_labels, cluster_strengths)
        """
        if self.embedding_model is None or self.umap_model is None or self.clusterer is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ fit()")
        
        print(f"üîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è {len(new_texts)} –Ω–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤...")
        
        # 1. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        new_embeddings = self.embedding_model.encode(
            new_texts,
            convert_to_numpy=True,
            show_progress_bar=True
        )
        
        # 2. UMAP
        new_embeddings_reduced = self.umap_model.transform(new_embeddings)
        
        # 3. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        labels, strengths = hdbscan.approximate_predict(
            self.clusterer,
            new_embeddings_reduced
        )
        
        print(f"‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        
        return labels, strengths
    
    def visualize(
        self,
        df: pd.DataFrame,
        save_path: str = 'clustering_visualization.png',
        figsize: Tuple[int, int] = (15, 10)
    ):
        """
        –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –≤ 2D
        
        Args:
            df: DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ (–¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'cluster_id')
            save_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            figsize: —Ä–∞–∑–º–µ—Ä —Ñ–∏–≥—É—Ä—ã
        """
        if self.embeddings is None:
            raise ValueError("–°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ fit()")
        
        print("üé® –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...")
        
        # UMAP –≤ 2D –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        umap_2d = UMAP(
            n_neighbors=15,
            n_components=2,
            min_dist=0.0,
            metric='cosine',
            random_state=self.random_seed
        )
        embeddings_2d = umap_2d.fit_transform(self.embeddings)
        
        # –ì—Ä–∞—Ñ–∏–∫
        plt.figure(figsize=figsize)
        
        cluster_labels = df['cluster_id'].values
        n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)
        n_noise = list(cluster_labels).count(-1)
        
        scatter = plt.scatter(
            embeddings_2d[:, 0],
            embeddings_2d[:, 1],
            c=cluster_labels,
            cmap='Spectral',
            s=5,
            alpha=0.6
        )
        
        plt.colorbar(scatter, label='Cluster ID')
        plt.title(
            f'–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤\n{n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, {n_noise} outliers',
            fontsize=14
        )
        plt.xlabel('UMAP 1')
        plt.ylabel('UMAP 2')
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        print(f"‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{save_path}'")
        plt.show()
    
    def get_cluster_samples(
        self,
        df: pd.DataFrame,
        n_samples: int = 5,
        text_col: str = 'text'
    ):
        """
        –í—ã–≤–æ–¥–∏—Ç –ø—Ä–∏–º–µ—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤ –∏–∑ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
        
        Args:
            df: DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            n_samples: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä
            text_col: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å —Ç–µ–∫—Å—Ç–æ–º
        """
        print("\n" + "="*80)
        print("–ü–†–ò–ú–ï–†–´ –¢–ï–ö–°–¢–û–í –ò–ó –ö–õ–ê–°–¢–ï–†–û–í")
        print("="*80)
        
        for cluster_id in sorted(df['cluster_id'].unique()):
            if cluster_id == -1:
                cluster_name = "–®–£–ú"
            else:
                cluster_name = f"–ö–õ–ê–°–¢–ï–† {cluster_id}"
            
            cluster_df = df[df['cluster_id'] == cluster_id]
            
            print(f"\nüìÅ {cluster_name} ({len(cluster_df)} —Ç–µ–∫—Å—Ç–æ–≤):")
            
            samples = cluster_df.head(n_samples)
            for i, (_, row) in enumerate(samples.iterrows(), 1):
                text = row[text_col]
                confidence = row.get('cluster_confidence', 0)
                
                # –û–±—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                text_short = text[:150] + "..." if len(text) > 150 else text
                print(f"  {i}. [{confidence:.2f}] {text_short}")
    
    def save_results(
        self,
        df: pd.DataFrame,
        csv_path: str = 'clustering_results.csv',
        excel_path: Optional[str] = None
    ):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Args:
            df: DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
            csv_path: –ø—É—Ç—å –¥–ª—è CSV —Ñ–∞–π–ª–∞
            excel_path: –ø—É—Ç—å –¥–ª—è Excel —Ñ–∞–π–ª–∞ (–µ—Å–ª–∏ –Ω—É–∂–µ–Ω)
        """
        # CSV
        df.to_csv(csv_path, index=False, encoding='utf-8')
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ '{csv_path}'")
        
        # Excel (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        if excel_path:
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                # –í—Å–µ –¥–∞–Ω–Ω—ã–µ
                df.to_excel(writer, sheet_name='–í—Å–µ –¥–∞–Ω–Ω—ã–µ', index=False)
                
                # –û—Ç–¥–µ–ª—å–Ω—ã–µ –ª–∏—Å—Ç—ã –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                for cluster_id in sorted(df['cluster_id'].unique()):
                    cluster_df = df[df['cluster_id'] == cluster_id]
                    
                    if cluster_id == -1:
                        sheet_name = '–®—É–º'
                    else:
                        sheet_name = f'–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}'
                    
                    cluster_df.to_excel(writer, sheet_name=sheet_name[:31], index=False)
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                stats = df.groupby('cluster_id').agg({
                    'cluster_id': 'count',
                    'cluster_confidence': 'mean'
                }).rename(columns={
                    'cluster_id': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ',
                    'cluster_confidence': '–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å'
                })
                stats.to_excel(writer, sheet_name='–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞')
            
            print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ '{excel_path}'")


# =============================================================================
# –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø
# =============================================================================

if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    data = {
        'id': list(range(1, 9)),
        'text': [
            '–ò—Å–ø—Ä–∞–≤–∏—Ç—å –±–∞–≥ —Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ —Å–∏—Å—Ç–µ–º–µ',
            '–ü—Ä–æ–±–ª–µ–º–∞ —Å –≤—Ö–æ–¥–æ–º —á–µ—Ä–µ–∑ OAuth —Ç—Ä–µ–±—É–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è',
            '–î–æ–±–∞–≤–∏—Ç—å –∫–Ω–æ–ø–∫—É —ç–∫—Å–ø–æ—Ä—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –≤ CSV —Ñ–æ—Ä–º–∞—Ç',
            '–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏—é –≤—ã–≥—Ä—É–∑–∫–∏ –æ—Ç—á—ë—Ç–æ–≤ –≤ Excel',
            '–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å SQL –∑–∞–ø—Ä–æ—Å –¥–ª—è –æ—Ç—á—ë—Ç–∞ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º',
            '–£—Å–∫–æ—Ä–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É –¥–∞—à–±–æ—Ä–¥–∞ —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏',
            '–û–±–Ω–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é API –¥–ª—è –Ω–æ–≤—ã—Ö endpoints',
            '–ù–∞–ø–∏—Å–∞—Ç—å README —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ'
        ]
    }
    
    df_input = pd.DataFrame(data)
    
    # –ò–ª–∏ –∑–∞–≥—Ä—É–∑–∏ –∏–∑ —Ñ–∞–π–ª–∞:
    # df_input = pd.read_csv('tasks.csv')
    # df_input = pd.read_excel('tasks.xlsx')
    
    # –°–æ–∑–¥–∞—ë–º pipeline
    # 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2' - —Å—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –±—ã—Å—Ç—Ä–æ
    # 'ai-forever/FRIDA' - –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–æ
    pipeline = TextClusteringPipeline(
        embedding_model_name='ai-forever/FRIDA'
    )
    
    # –û–±—É—á–∞–µ–º (fit)
    df_result = pipeline.fit(
        df_input,
        id_col='id',
        text_col='text',
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å:
        min_cluster_size=2,  # –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        min_samples=1,
        n_components=5
    )
    
    # –ü—Ä–æ—Å–º–æ—Ç—Ä –ø—Ä–∏–º–µ—Ä–æ–≤
    pipeline.get_cluster_samples(df_result, n_samples=3)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    pipeline.visualize(df_result)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    pipeline.save_results(
        df_result,
        csv_path='results.csv',
        excel_path='results.xlsx'
    )
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –Ω–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤
    new_tasks = [
        '–ü–æ—á–∏–Ω–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É —Å –ª–æ–≥–∏–Ω–æ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è',
        '–î–æ–±–∞–≤–∏—Ç—å —ç–∫—Å–ø–æ—Ä—Ç –≤ JSON —Ñ–æ—Ä–º–∞—Ç'
    ]
    labels, confidences = pipeline.predict(new_tasks)
    
    print("\nüîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á:")
    for task, label, conf in zip(new_tasks, labels, confidences):
        print(f"  '{task}' ‚Üí –ö–ª–∞—Å—Ç–µ—Ä {label} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {conf:.2f})")